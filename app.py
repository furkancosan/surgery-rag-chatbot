# app.py
"""
Web tabanlÄ± Plastik Cerrahi RAG Chatbot arayÃ¼zÃ¼.

Ã–zellikler:
- ChatGPT benzeri sohbet arayÃ¼zÃ¼ (soru-cevaplar yukarÄ± doÄŸru birikir)
- Sol sidebar'da:
    - Ãœstte: Model AyarlarÄ± (temperature, retriever_k, max_tokens)
    - Hemen altÄ±nda: PDF yÃ¼kleme + indekse ekleme + dokÃ¼man filtresi
- Arkada:
    - KullanÄ±cÄ±nÄ±n yÃ¼klediÄŸi PDF'ler â†’ FAISS vektÃ¶r indeksi
    - Groq LLM + RAG zinciri
    - FAISS + kaynak dokÃ¼man listesi disk Ã¼zerinde saklanÄ±r
"""

import os
import json
import hashlib
from pathlib import Path

import streamlit as st

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from config import INDEX_YOLU, EMBEDDING_MODEL
from data_processor import load_and_chunk_data, create_vector_store, extend_vector_store
from rag_chain import setup_rag_chain
from utils.logging_utils import setup_logger

logger = setup_logger(__name__)

# ============================================
# 1) Streamlit genel ayarlarÄ±
# ============================================
st.set_page_config(
    page_title="Surgery RAG Chatbot",
    page_icon="ğŸ©º",
    layout="wide",
)


# ============================================
# 2) YardÄ±mcÄ± fonksiyonlar
# ============================================

def compute_file_hash(file_bytes: bytes) -> str:
    """
    Dosya iÃ§eriÄŸinden MD5 hash Ã¼retir.
    AynÄ± iÃ§erikteki PDF'lerin tekrar indekse eklenmesini engellemek iÃ§in kullanÄ±lÄ±r.
    (Sadece o oturum boyunca geÃ§erlidir; disk Ã¼zerinde hash saklamÄ±yoruz.)
    """
    return hashlib.md5(file_bytes).hexdigest()


def load_vector_store_from_disk():
    """
    Diskte daha Ã¶nce kaydedilmiÅŸ bir FAISS vektÃ¶r indeksi varsa yÃ¼kler.

    DÃ¶nÃ¼ÅŸ:
        - FAISS vector_store veya
        - None (indeks bulunamazsa veya hata oluÅŸursa)
    """
    index_path = Path(INDEX_YOLU)

    if not index_path.exists():
        logger.info("FAISS indeks klasÃ¶rÃ¼ bulunamadÄ±: %s", index_path)
        return None

    try:
        logger.info("Diskten FAISS vektÃ¶r indeksi yÃ¼kleniyor: %s", index_path)
        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        vector_store = FAISS.load_local(
            str(index_path),
            embeddings,
            allow_dangerous_deserialization=True,
        )
        logger.info("FAISS vektÃ¶r indeksi baÅŸarÄ±yla yÃ¼klendi.")
        return vector_store
    except Exception as e:
        logger.exception("FAISS indeksi yÃ¼klenirken hata oluÅŸtu: %s", e)
        return None


def save_vector_store_to_disk(vector_store):
    """
    Mevcut FAISS vektÃ¶r indeksini diske kaydeder.
    """
    if vector_store is None:
        return

    index_path = Path(INDEX_YOLU)
    index_path.mkdir(parents=True, exist_ok=True)

    try:
        vector_store.save_local(str(index_path))
        logger.info("FAISS vektÃ¶r indeksi diske kaydedildi: %s", index_path)
    except Exception as e:
        logger.exception("FAISS indeksi kaydedilirken hata oluÅŸtu: %s", e)


def load_available_sources():
    """
    Daha Ã¶nce kaydedilmiÅŸ kaynak dokÃ¼man listesini (dosya adlarÄ±nÄ±) diskten okur.

    DÃ¶nÃ¼ÅŸ:
        - List[str] (dokÃ¼man adlarÄ±) veya boÅŸ liste
    """
    meta_path = Path(INDEX_YOLU) / "sources.json"
    if not meta_path.exists():
        logger.info("sources.json bulunamadÄ±, boÅŸ liste ile baÅŸlanacak.")
        return []

    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            logger.info("sources.json yÃ¼klendi. DokÃ¼man sayÄ±sÄ±: %d", len(data))
            return data
        logger.warning("sources.json beklenen formatta deÄŸil, boÅŸ liste dÃ¶nÃ¼lecek.")
        return []
    except Exception as e:
        logger.exception("sources.json yÃ¼klenirken hata oluÅŸtu: %s", e)
        return []


def save_available_sources():
    """
    Mevcut available_sources listesini INDEX_YOLU iÃ§ine sources.json olarak kaydeder.
    """
    meta_dir = Path(INDEX_YOLU)
    meta_dir.mkdir(parents=True, exist_ok=True)
    meta_path = meta_dir / "sources.json"

    try:
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(
                st.session_state["available_sources"],
                f,
                ensure_ascii=False,
                indent=2,
            )
        logger.info("available_sources metadata kaydedildi: %s", meta_path)
    except Exception as e:
        logger.exception("available_sources metadata kaydedilirken hata oluÅŸtu: %s", e)


def rebuild_rag_chain():
    """
    Mevcut vector_store + UI'deki model ayarlarÄ±na gÃ¶re
    RAG zincirini (retrieval_chain) yeniden kurar.
    """
    vector_store = st.session_state["vector_store"]
    if vector_store is None:
        st.session_state["retrieval_chain"] = None
        logger.warning("RAG zinciri kurulamadÄ±: vector_store None.")
        return

    temperature = st.session_state["temperature"]
    retriever_k = st.session_state["retriever_k"]
    max_tokens = st.session_state["max_tokens"]

    logger.info(
        "RAG zinciri yeniden kuruluyor (temperature=%.2f, k=%d, max_tokens=%d)",
        temperature,
        retriever_k,
        max_tokens,
    )

    retrieval_chain = setup_rag_chain(
        vector_store,
        temperature=temperature,
        retriever_k_override=retriever_k,
        max_tokens=max_tokens,
    )
    st.session_state["retrieval_chain"] = retrieval_chain
    logger.info("RAG zinciri baÅŸarÄ±yla yeniden kuruldu.")


def add_uploaded_pdfs_to_index(uploaded_files):
    """
    Sidebar'dan yÃ¼klenen PDF dosyalarÄ±nÄ±:
    - Diske kaydeder
    - Chunk'lara bÃ¶ler
    - EÄŸer yeni iÃ§erikse FAISS indeksine ekler
    - GÃ¼ncel FAISS indeksini ve kaynak dokÃ¼man listesini diske kaydeder
    """
    if not uploaded_files:
        st.warning("Ã–nce en az bir PDF seÃ§melisin.")
        return

    upload_dir = Path("uploaded_pdfs")
    upload_dir.mkdir(parents=True, exist_ok=True)

    all_new_docs = []
    total_new_docs = 0
    sources_changed = False

    for up_file in uploaded_files:
        file_bytes = up_file.getvalue()
        file_hash = compute_file_hash(file_bytes)

        # AynÄ± iÃ§erik daha Ã¶nce indekslenmiÅŸse (bu oturumda) atla
        if file_hash in st.session_state["indexed_hashes"]:
            st.info(f"'{up_file.name}' iÃ§eriÄŸi zaten indekse eklenmiÅŸ, tekrar eklenmedi.")
            logger.info("AynÄ± iÃ§erik hash ile tespit edildi, atlandÄ±: %s", up_file.name)
            continue

        # Yeni iÃ§erik â†’ hash'i kaydet
        st.session_state["indexed_hashes"].add(file_hash)

        # DosyayÄ± diske yaz
        file_path = upload_dir / up_file.name
        with open(file_path, "wb") as f:
            f.write(file_bytes)

        logger.info("Yeni PDF yÃ¼klendi ve kaydedildi: %s", file_path)

        # PDF'i chunk'lara bÃ¶l
        docs_new = load_and_chunk_data(str(file_path), source_name=up_file.name)
        if docs_new:
            all_new_docs.extend(docs_new)
            total_new_docs += len(docs_new)

            if up_file.name not in st.session_state["available_sources"]:
                st.session_state["available_sources"].append(up_file.name)
                sources_changed = True
        else:
            logger.warning("PDF'den dokÃ¼man Ã¼retilemedi: %s", file_path)

    if not all_new_docs:
        st.warning("YÃ¼klenen dosyalardan yeni iÃ§erik eklenmedi (hepsi daha Ã¶nce indekslenmiÅŸ olabilir).")
        return

    # Yeni dokÃ¼manlarÄ± FAISS indeksine ekle / oluÅŸtur
    if st.session_state["vector_store"] is None:
        vector_store = create_vector_store(all_new_docs)
    else:
        vector_store = extend_vector_store(st.session_state["vector_store"], all_new_docs)

    if vector_store is None:
        st.error("VektÃ¶r deposu oluÅŸturulamadÄ±/gÃ¼ncellenemedi.")
        return

    st.session_state["vector_store"] = vector_store

    # Vector store deÄŸiÅŸti â†’ diske kaydet + RAG zincirini yeniden kur
    save_vector_store_to_disk(vector_store)

    if sources_changed:
        save_available_sources()

    rebuild_rag_chain()

    st.success(f"âœ… Yeni dokÃ¼manlar indekse eklendi. Toplam eklenen parÃ§a sayÄ±sÄ±: {total_new_docs}")
    logger.info(
        "Yeni dokÃ¼manlar indekse eklendi. Toplam yeni parÃ§a: %d, available_sources: %s",
        total_new_docs,
        st.session_state["available_sources"],
    )


def render_assistant_meta(meta: dict):
    """
    Assistant cevabÄ±nÄ±n altÄ±na RAG meta bilgisini (confidence, mode, sources vs.)
    profesyonel ve sade bir ÅŸekilde gÃ¶sterir.
    """
    if not meta:
        return

    confidence = meta.get("confidence")
    mode = meta.get("mode")
    pages = meta.get("pages", [])
    sources = meta.get("sources", [])
    source_count = meta.get("source_count")

    with st.expander("ğŸ“Š Cevap Ã–zeti (RAG)", expanded=False):
        if confidence is not None:
            pct = int(confidence * 100)
            if confidence >= 0.7:
                label = "YÃ¼ksek gÃ¼ven"
                icon = "ğŸŸ¢"
            elif confidence >= 0.4:
                label = "Orta gÃ¼ven"
                icon = "ğŸŸ¡"
            else:
                label = "DÃ¼ÅŸÃ¼k gÃ¼ven"
                icon = "ğŸ”´"
            st.markdown(f"**RAG GÃ¼ven Skoru:** {icon} {pct}% ({label})")

        if mode:
            if mode == "pdf_strong":
                mode_text = "Cevap bÃ¼yÃ¼k oranda referans PDF iÃ§eriÄŸine dayanÄ±yor."
            elif mode == "hybrid":
                mode_text = "Cevap referans PDF + genel tÄ±bbi bilgiyi birlikte kullanÄ±yor."
            elif mode == "general":
                mode_text = "Referans PDF yetersiz, cevap daha Ã§ok genel tÄ±bbi bilgiye dayanÄ±yor."
            elif mode == "no_docs":
                mode_text = "Ä°ndekste henÃ¼z referans dokÃ¼man yok."
            else:
                mode_text = ""
            st.markdown(f"**Ã‡alÄ±ÅŸma Modu:** `{mode}`  \n{mode_text}")

        if pages:
            human_pages = [p + 1 for p in pages if isinstance(p, int)]
            st.markdown(f"**Kaynak sayfalar (0-based index):** {', '.join(map(str, pages))}")
            if human_pages:
                st.markdown(f"**Kaynak sayfalar (PDF sayfa numarasÄ±):** {', '.join(map(str, human_pages))}")

        if source_count is not None:
            st.markdown(f"**KullanÄ±lan kaynak parÃ§a sayÄ±sÄ±:** {source_count}")

        if sources:
            st.markdown("**Kaynak dokÃ¼manlar:**")
            for s in sources:
                name = s.get("name")
                pgs = s.get("pages", [])
                if name:
                    if pgs:
                        st.markdown(f"- `{name}` â†’ sayfalar: {', '.join(map(str, pgs))}")
                    else:
                        st.markdown(f"- `{name}`")


# ============================================
# 3) Session state baÅŸlangÄ±Ã§ deÄŸerleri
#    (helper fonksiyonlardan yararlanarak)
# ============================================

# Model ince ayarlarÄ± (fine-tuning parametreleri)
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.0  # config'teki defaultunla eÅŸlenebilir

if "retriever_k" not in st.session_state:
    st.session_state["retriever_k"] = 5

if "max_tokens" not in st.session_state:
    st.session_state["max_tokens"] = 768  # makul bir baÅŸlangÄ±Ã§ deÄŸeri

# FAISS vektÃ¶r indeksi
if "vector_store" not in st.session_state:
    st.session_state["vector_store"] = load_vector_store_from_disk()

# RAG zinciri
if "retrieval_chain" not in st.session_state:
    if st.session_state["vector_store"] is not None:
        # Vector store diskte vardÄ± â†’ aÃ§Ä±lÄ±ÅŸta direkt RAG zinciri kur
        rebuild_rag_chain()
    else:
        st.session_state["retrieval_chain"] = None

# YÃ¼klÃ¼ dokÃ¼man isimleri (diskten oku)
if "available_sources" not in st.session_state:
    st.session_state["available_sources"] = load_available_sources()

# AynÄ± PDF iÃ§eriklerini oturum iÃ§inde yakalamak iÃ§in hash set'i
if "indexed_hashes" not in st.session_state:
    st.session_state["indexed_hashes"] = set()

# Sohbet geÃ§miÅŸi
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []


# ============================================
# 4) Sidebar: Ayarlar + DokÃ¼man yÃ¶netimi
# ============================================

with st.sidebar:
    st.markdown("### âš™ï¸ Model AyarlarÄ±")

    st.session_state["temperature"] = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=1.0,
        value=float(st.session_state["temperature"]),
        step=0.05,
        help="0.0 â†’ daha deterministik, 1.0 â†’ daha yaratÄ±cÄ±/daÄŸÄ±nÄ±k.",
    )

    st.session_state["retriever_k"] = st.slider(
        "Retriever k",
        min_value=1,
        max_value=10,
        value=int(st.session_state["retriever_k"]),
        step=1,
        help="Sorgu baÅŸÄ±na FAISS'ten Ã§ekilecek maksimum parÃ§a sayÄ±sÄ±.",
    )

    st.session_state["max_tokens"] = st.slider(
        "max_tokens",
        min_value=128,
        max_value=2048,
        value=int(st.session_state["max_tokens"]),
        step=64,
        help="Modelin Ã¼reteceÄŸi maksimum token sayÄ±sÄ±.",
    )

    if st.button("AyarlarÄ± Uygula", type="primary", use_container_width=True):
        if st.session_state["vector_store"] is None:
            st.warning("Ã–nce en az bir PDF yÃ¼kleyip indekse eklemelisin.")
        else:
            rebuild_rag_chain()
            st.success("Model ayarlarÄ± gÃ¼ncellendi ve RAG zinciri yeniden kuruldu.")

    st.markdown("---")
    st.markdown("### ğŸ“ Kaynak DokÃ¼manlar")

    uploaded_files = st.file_uploader(
        "PDF yÃ¼kle",
        type=["pdf"],
        accept_multiple_files=True,
    )

    if st.button("Ä°ndekse ekle â•", use_container_width=True):
        if not uploaded_files:
            st.warning("Ã–nce en az bir PDF seÃ§melisin.")
        else:
            # YÃ¼kleme + indeksleme sÄ±rasÄ±nda spinner gÃ¶ster
            with st.spinner("DokÃ¼manlar indekse ekleniyor..."):
                add_uploaded_pdfs_to_index(uploaded_files)

    st.markdown("**YÃ¼klÃ¼ dokÃ¼manlar:**")
    if st.session_state["available_sources"]:
        for name in st.session_state["available_sources"]:
            st.markdown(f"- `{name}`")
    else:
        st.caption("HenÃ¼z dokÃ¼man yÃ¼klenmedi.")

    st.markdown("---")

    # Cevapta hangi dokÃ¼manlarÄ±n kullanÄ±labileceÄŸini seÃ§ (allowed_sources)
    allowed_sources = st.multiselect(
        "Cevaplarda kullanÄ±lacak dokÃ¼manlar",
        options=st.session_state["available_sources"],
        default=st.session_state["available_sources"],
        help="BoÅŸ bÄ±rakÄ±rsan tÃ¼m dokÃ¼manlar kullanÄ±labilir.",
    )


# ============================================
# 5) Ana bÃ¶lÃ¼m: BaÅŸlÄ±k + Chat arayÃ¼zÃ¼
# ============================================

st.markdown(
    """
    <style>
    .block-container {
        padding-top: 1.5rem;
        padding-bottom: 1.5rem;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown("## ğŸ©º Surgery RAG Chatbot")
st.caption("YÃ¼klediÄŸin cerrahi PDF'ler Ã¼zerinde Ã§alÄ±ÅŸan, Groq destekli soru-cevap asistanÄ±.")

# Ã–nce geÃ§miÅŸ mesajlarÄ± render et
for msg in st.session_state["chat_history"]:
    role = msg.get("role", "assistant")
    content = msg.get("content", "")
    meta = msg.get("meta", {})

    with st.chat_message(role):
        st.markdown(content)
        if role == "assistant" and meta:
            render_assistant_meta(meta)

# En altta chat input
user_input = st.chat_input("Sorunu buraya yaz (TÃ¼rkÃ§e veya Ä°ngilizce)...")

if user_input:
    # 1) KullanÄ±cÄ± mesajÄ±nÄ± kaydet ve gÃ¶ster
    st.session_state["chat_history"].append(
        {"role": "user", "content": user_input, "meta": {}}
    )
    with st.chat_message("user"):
        st.markdown(user_input)

    # 2) Assistant cevabÄ±nÄ± Ã¼ret
    retrieval_chain = st.session_state["retrieval_chain"]

    # allowed_sources boÅŸsa None gÃ¶nder â†’ rag_chain tÃ¼m dokÃ¼manlardan seÃ§sin
    allowed_sources_param = allowed_sources or None

    if retrieval_chain is None:
        # HenÃ¼z indeks yok veya RAG kurulmamÄ±ÅŸ
        assistant_text = (
            "HenÃ¼z herhangi bir PDF indekse eklenmemiÅŸ gÃ¶rÃ¼nÃ¼yor. "
            "Sol taraftan bir veya daha fazla PDF yÃ¼kleyip 'Ä°ndekse ekle' butonuna bastÄ±ktan sonra soru sorabilirsin."
        )
        meta = {"mode": "no_docs", "confidence": 0.0}
    else:
        try:
            resp = retrieval_chain.invoke(
                {
                    "input": user_input,
                    "allowed_sources": allowed_sources_param,
                }
            )

            # Beklenen format:
            # {
            #   "answer": str,
            #   "pages": [...],
            #   "confidence": float,
            #   "source_count": int,
            #   "mode": "...",
            #   "sources": [...]
            # }
            if isinstance(resp, dict):
                answer = resp.get("answer", "")
                pages = resp.get("pages", [])
                confidence = resp.get("confidence", 0.0)
                mode = resp.get("mode", "none")
                source_count = resp.get("source_count", 0)
                sources = resp.get("sources", [])
            else:
                answer = str(resp)
                pages = []
                confidence = 0.0
                mode = "none"
                source_count = 0
                sources = []

            assistant_text = answer
            meta = {
                "pages": pages,
                "confidence": confidence,
                "mode": mode,
                "source_count": source_count,
                "sources": sources,
            }

        except Exception as e:
            logger.exception("Soru iÅŸlenirken hata oluÅŸtu: %s", e)
            assistant_text = (
                "Soru iÅŸlenirken bir hata oluÅŸtu. LoglarÄ± kontrol etmen gerekebilir.\n\n"
                f"Hata: {e}"
            )
            meta = {"mode": "error", "confidence": 0.0}

    # 3) Assistant mesajÄ±nÄ± kaydet ve gÃ¶ster
    st.session_state["chat_history"].append(
        {"role": "assistant", "content": assistant_text, "meta": meta}
    )

    with st.chat_message("assistant"):
        st.markdown(assistant_text)
        render_assistant_meta(meta)
