import os
import logging
from pathlib import Path
from typing import Dict, Any

from langchain_groq import ChatGroq
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import SystemMessage, HumanMessage
from sentence_transformers import CrossEncoder

from config import (
    GROQ_MODEL,
    TEMPERATURE,
    SYSTEM_PROMPT,
    RETRIEVER_K,
    RERANK_TOP_K,  # hÃ¢lÃ¢ import ediyoruz; istersen ileride kaldÄ±rabilirsin
    RERANK_MODEL_NAME,
    USE_TURKISH_PIPELINE,
    TRANSLATION_MODEL,
    DEBUG,
    CONFIDENCE_HIGH,
    CONFIDENCE_MEDIUM,
    ENABLE_CLOSING_SUGGESTION,
)
from utils.logging_utils import setup_logger

"""
rag_chain.py
------------
Groq LLM + FAISS vektÃ¶r indeksi + CrossEncoder reranker + (opsiyonel) TR/EN
Ã§eviri pipeline'Ä±nÄ± bir araya getirerek tam fonksiyonel bir RAG zinciri kurar.

Ana fonksiyon: setup_rag_chain(vector_store, ...)
DÃ¶nÃ¼ÅŸ: RunnableLambda (invoke({...}) ile Ã§aÄŸrÄ±labilir)

GiriÅŸ formatÄ±:
    {
        "input": "<kullanÄ±cÄ± sorusu>",
        "allowed_sources": ["doc1.pdf", "doc2.pdf"]  # opsiyonel (None = tÃ¼m dokÃ¼manlar)
    }

Ã‡Ä±kÄ±ÅŸ formatÄ±:
    {
        "answer": "<cevap + sohbet kapanÄ±ÅŸÄ±>",
        "pages": [page_index_listesi (0-based)],
        "confidence": 0.0 - 1.0,
        "source_count": int,
        "mode": "pdf_strong" | "hybrid" | "general" | "none",
        "sources": [
            {"name": "doc1.pdf", "pages": [1, 2, 5]},
            {"name": "ek_dokuman.pdf", "pages": [3]}
        ]
    }
"""

logger = setup_logger(__name__)
if DEBUG:
    logger.setLevel(logging.DEBUG)

# Groq API anahtarÄ± ortam deÄŸiÅŸkeninden okunur
GROQ_API_KEY = os.getenv("GROQ_API_KEY")


def is_turkish(text: str) -> bool:
    """
    Metinde TÃ¼rkÃ§e karakter olup olmadÄ±ÄŸÄ±nÄ± basitÃ§e kontrol eder.

    AmaÃ§:
    - Soru TÃ¼rkÃ§e mi Ä°ngilizce mi, hÄ±zlÄ± ve hafif bir heuristik ile anlamak
    - EÄŸer TÃ¼rkÃ§e ise Ã§eviri pipeline'Ä±nÄ± devreye sokmak
    """
    tr_chars = "Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžÄ°Ã–ÅžÃœ"
    return any(c in tr_chars for c in text)


def setup_rag_chain(
    vector_store,
    temperature: float | None = None,
    retriever_k_override: int | None = None,
    max_tokens: int | None = None,
):
    """
    Groq + FAISS ile RAG zincirini kurar.

    Parametreler:
        vector_store: FAISS vektÃ¶r deposu
        temperature: (opsiyonel) LLM iÃ§in sÄ±caklÄ±k. None ise config.TEMPERATURE kullanÄ±lÄ±r.
        retriever_k_override: (opsiyonel) FAISS'ten Ã§ekilecek parÃ§a sayÄ±sÄ±. None ise config.RETRIEVER_K.
        max_tokens: (opsiyonel) LLM'in Ã¼reteceÄŸi maksimum token sayÄ±sÄ±.

    Not:
        - effective_retriever_k hem FAISS'ten Ã§ekilen aday sayÄ±sÄ±nÄ±, hem de
          CrossEncoder sonrasÄ± LLM'e gÃ¶nderilecek nihai dokÃ¼man sayÄ±sÄ±nÄ± sÄ±nÄ±rlar.
    """
    if vector_store is None:
        raise ValueError(
            "VektÃ¶r deposu (vector_store) None geldi. "
            "Ã–nce indeksin baÅŸarÄ±yla yÃ¼klendiÄŸinden/oluÅŸturulduÄŸundan emin olun."
        )

    if not GROQ_API_KEY:
        raise RuntimeError(
            "Groq API anahtarÄ± bulunamadÄ±. LÃ¼tfen 'GROQ_API_KEY' ortam deÄŸiÅŸkenini ayarlayÄ±n."
        )

    effective_temperature = temperature if temperature is not None else TEMPERATURE
    effective_retriever_k = (
        retriever_k_override if retriever_k_override is not None else RETRIEVER_K
    )

    logger.info(
        "RAG zinciri kuruluyor (Model: %s, temperature=%.2f, retriever_k=%d, max_tokens=%s)...",
        GROQ_MODEL,
        effective_temperature,
        effective_retriever_k,
        str(max_tokens),
    )

    # 1) Ana LLM (hem RAG cevabÄ± hem de kapanÄ±ÅŸ Ã¶nerisi iÃ§in kullanÄ±lacak)
    llm_main_kwargs: Dict[str, Any] = {}
    if max_tokens is not None:
        llm_main_kwargs["max_tokens"] = max_tokens

    llm_main = ChatGroq(
        temperature=effective_temperature,
        model_name=GROQ_MODEL,
        api_key=GROQ_API_KEY,
        **llm_main_kwargs,
    )

    # 2) Reranker Modelini YÃ¼kleme (CrossEncoder)
    logger.info("Reranker CrossEncoder modeli yÃ¼kleniyor: %s", RERANK_MODEL_NAME)
    reranker = CrossEncoder(RERANK_MODEL_NAME)

    # 3) Ã‡eviri LLM'i (TÃ¼rkÃ§e <-> Ä°ngilizce)
    if USE_TURKISH_PIPELINE:
        llm_translate = ChatGroq(
            temperature=0,
            model_name=TRANSLATION_MODEL or GROQ_MODEL,
            api_key=GROQ_API_KEY,
        )
        logger.debug(
            "TÃ¼rkÃ§e Ã§eviri pipeline aktif. Model: %s",
            TRANSLATION_MODEL or GROQ_MODEL,
        )
    else:
        llm_translate = None
        logger.debug("TÃ¼rkÃ§e Ã§eviri pipeline devre dÄ±ÅŸÄ±.")

    # --- Ã‡eviri yardÄ±mcÄ± fonksiyonlarÄ± ---

    def translate_tr_to_en(question_tr: str) -> str:
        """
        TÃ¼rkÃ§e soruyu Ä°ngilizceye Ã§evirir.
        """
        if not llm_translate:
            return question_tr

        messages = [
            SystemMessage(
                content=(
                    "You are a professional translator. Translate the user's question "
                    "from Turkish to natural English suitable for a plastic surgery textbook context. "
                    "Only return the translated question, nothing else."
                )
            ),
            HumanMessage(content=question_tr),
        ]
        resp = llm_translate.invoke(messages)
        translated = getattr(resp, "content", str(resp)).strip()
        logger.debug("TR â†’ EN Ã§eviri tamamlandÄ±.")
        return translated

    def translate_en_to_tr(answer_en: str) -> str:
        """
        Ä°ngilizce cevabÄ± TÃ¼rkÃ§eye Ã§evirir.
        """
        if not llm_translate:
            return answer_en

        messages = [
            SystemMessage(
                content=(
                    "You are a professional translator. Translate the following answer "
                    "from English to natural, fluent Turkish for a medical professional. "
                    "Only return the translated text, nothing else."
                )
            ),
            HumanMessage(content=answer_en),
        ]
        resp = llm_translate.invoke(messages)
        translated = getattr(resp, "content", str(resp)).strip()
        logger.debug("EN â†’ TR Ã§eviri tamamlandÄ±.")
        return translated

    # --- Sohbet tarzÄ± kapanÄ±ÅŸ cÃ¼mlesi Ã¼retici ---

    def generate_closing_note(
        original_question: str,
        final_answer: str,
        mode: str,
        original_is_turkish: bool,
    ) -> str:
        """
        CevabÄ±n sonuna eklenecek, sohbeti devam ettiren 1-2 cÃ¼mle Ã¼retir.
        """
        if not ENABLE_CLOSING_SUGGESTION:
            return ""

        lang_instr = (
            "CÃ¼mleyi TÃ¼rkÃ§e yaz."
            if original_is_turkish
            else "Write the sentences in English."
        )

        mode_info = {
            "pdf_strong": "The answer was mainly based on the reference textbook excerpt.",
            "hybrid": "The answer combined the reference textbook and general medical knowledge.",
            "general": "The answer was based on general medical knowledge, not the textbook.",
        }.get(mode, "")

        sys_msg = (
            "You are helping a plastic surgery learner in an ongoing chat. "
            "Based on the user's question and your answer, write 1-2 short sentences that:\n"
            "- Invite the user to continue the conversation,\n"
            "- Optionally suggest one or two related subtopics you could explain next,\n"
            "- Sound natural and friendly, not like a bullet list.\n"
            f"{lang_instr}\n\n"
            "Do not repeat the full answer. Do not list multiple questions; just write a small closing message.\n"
            f"Context about how the answer was generated: {mode_info}"
        )

        messages = [
            SystemMessage(content=sys_msg),
            HumanMessage(
                content=f"KullanÄ±cÄ±nÄ±n sorusu:\n{original_question}\n\nVerilen cevap:\n{final_answer}"
            ),
        ]

        resp = llm_main.invoke(messages)
        text = getattr(resp, "content", str(resp)).strip()
        logger.debug("KapanÄ±ÅŸ Ã¶nerisi Ã¼retildi.")
        return text

    # --- AsÄ±l RAG fonksiyonu ---

    def rag_fn(inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ana RAG iÅŸlevi.

        inputs:
            - input: str (zorunlu)
            - allowed_sources: List[str] | None (opsiyonel)

        allowed_sources:
            - None veya []         â†’ tÃ¼m dokÃ¼manlar kullanÄ±labilir
            - ["Grabb.pdf", ...]   â†’ sadece bu source_name'lere sahip chunk'lar kullanÄ±lacak
        """
        question_raw = inputs.get("input", "").strip()
        allowed_sources = inputs.get("allowed_sources")  # List[str] veya None

        if not question_raw:
            logger.warning("BoÅŸ soru alÄ±ndÄ±.")
            return {
                "answer": "Soru boÅŸ geldi.",
                "pages": [],
                "confidence": 0.0,
                "source_count": 0,
                "mode": "none",
                "sources": [],
            }

        logger.debug("Allowed_sources: %s", allowed_sources)

        # 1) Dil tespiti & gerekirse EN'e Ã§eviri
        original_is_turkish = USE_TURKISH_PIPELINE and is_turkish(question_raw)

        if original_is_turkish:
            logger.debug("Soru TÃ¼rkÃ§e algÄ±landÄ±, Ä°ngilizceye Ã§evriliyor...")
            question_en = translate_tr_to_en(question_raw)
        else:
            question_en = question_raw

        logger.debug("RAG iÃ§in kullanÄ±lacak soru (EN): %s", question_en)

        # 2) FAISS ile benzerlik + skor (confidence)
        try:
            results = vector_store.similarity_search_with_relevance_scores(
                question_en,
                k=effective_retriever_k,
            )
        except AttributeError:
            logger.debug(
                "similarity_search_with_relevance_scores bulunamadÄ±, fallback'e geÃ§iliyor."
            )
            docs_only = vector_store.similarity_search(
                question_en, k=effective_retriever_k
            )
            results = [(doc, 0.0) for doc in docs_only]

        docs = [doc for doc, _ in results]
        scores = [float(score) for _, score in results] if results else []
        confidence = max(scores) if scores else 0.0

        logger.debug(
            "FAISS'ten %d dokÃ¼man dÃ¶ndÃ¼. Max skor (confidence): %.3f",
            len(docs),
            confidence,
        )

        # 2.5) CrossEncoder ile reranking
        reranked_docs = docs
        if docs:
            try:
                pairs = [[question_en, doc.page_content] for doc in docs]
                rerank_scores = reranker.predict(pairs)
                doc_score_pairs = list(zip(docs, rerank_scores))

                doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
                # ðŸ”¹ Burada artÄ±k RERANK_TOP_K yerine effective_retriever_k kullanÄ±yoruz
                top_k = min(effective_retriever_k, len(doc_score_pairs))
                reranked_docs = [doc for doc, _ in doc_score_pairs[:top_k]]

                logger.debug(
                    "Reranker: Ä°lk FAISS sonuÃ§ sayÄ±sÄ±: %d, top_k (LLM'e giden parÃ§a sayÄ±sÄ±): %d",
                    len(docs),
                    top_k,
                )
                for i, (doc, s) in enumerate(doc_score_pairs[:top_k], start=1):
                    page = (
                        doc.metadata.get("page")
                        if isinstance(doc.metadata, dict)
                        else None
                    )
                    logger.debug("  %d. skor=%.3f (page=%s)", i, s, page)
            except Exception as e:
                logger.warning("Reranking sÄ±rasÄ±nda hata oluÅŸtu: %s", e)

        # 2.6) allowed_sources filtresini uygula (varsa)
        if allowed_sources:
            allowed_set = set(allowed_sources)
            filtered_docs = []
            for doc in reranked_docs:
                md = doc.metadata if isinstance(doc.metadata, dict) else {}
                src_name = md.get("source_name") or md.get("source")
                if src_name and Path(src_name).name in allowed_set:
                    filtered_docs.append(doc)

            logger.debug(
                "allowed_sources filtresi uygulandÄ±. Ã–nce: %d dokÃ¼man, sonra: %d dokÃ¼man.",
                len(reranked_docs),
                len(filtered_docs),
            )
            reranked_docs = filtered_docs

            # HiÃ§ dokÃ¼man kalmazsa, direkt bilgilendirici cevap dÃ¶n
            if not reranked_docs:
                return {
                    "answer": (
                        "SeÃ§tiÄŸin dokÃ¼man(lar) iÃ§inde bu soruya dair yeterli bilgi bulamadÄ±m. "
                        "Ä°stersen tÃ¼m dokÃ¼manlara aÃ§Ä±k ÅŸekilde tekrar deneyebilirsin."
                    ),
                    "pages": [],
                    "confidence": 0.0,
                    "source_count": 0,
                    "mode": "none",
                    "sources": [],
                }

        # 3) Mode belirleme (confidence FAISS skorlarÄ±ndan geliyor)
        if confidence >= CONFIDENCE_HIGH:
            mode = "pdf_strong"
        elif confidence >= CONFIDENCE_MEDIUM:
            mode = "hybrid"
        else:
            mode = "general"

        # 4) Sayfa numaralarÄ±nÄ± topla (0-based)
        page_numbers = sorted(
            {
                doc.metadata.get("page")
                for doc in reranked_docs
                if isinstance(doc.metadata, dict) and "page" in doc.metadata
            }
        )

        # 4.1) Kaynak dokÃ¼man + sayfalar bilgisini Ã§Ä±kar
        sources_dict: Dict[str, set[int]] = {}
        for doc in reranked_docs:
            md = doc.metadata if isinstance(doc.metadata, dict) else {}
            raw_source_name = md.get("source_name") or md.get("source")

            if not raw_source_name:
                continue

            source_name = Path(raw_source_name).name
            page_idx = md.get("page")

            if isinstance(page_idx, int):
                human_page = page_idx + 1
            else:
                human_page = None

            if source_name not in sources_dict:
                sources_dict[source_name] = set()

            if human_page is not None:
                sources_dict[source_name].add(human_page)

        sources_list = []
        for name, pages_set in sources_dict.items():
            pages_sorted = sorted(pages_set)
            sources_list.append(
                {
                    "name": name,
                    "pages": pages_sorted,
                }
            )

        context_text = "\n\n".join(doc.page_content for doc in reranked_docs)

        logger.debug("Reranker sonrasÄ± kullanÄ±lan dokÃ¼man sayÄ±sÄ±: %d", len(reranked_docs))
        logger.debug("Kaynak sayfalar (0-based index): %s", page_numbers)
        logger.debug("RAG Ã§alÄ±ÅŸma modu: %s, confidence: %.3f", mode, confidence)
        logger.debug("Kaynak dokÃ¼man sayÄ±sÄ±: %d", len(sources_list))

        # 5) Mode'a gÃ¶re system prompt ve context
        if mode == "pdf_strong":
            system_content = SYSTEM_PROMPT.format(context=context_text)

        elif mode == "hybrid":
            system_content = (
                "You are a plastic surgery educational assistant. You have access to:\n"
                "1) A reference textbook excerpt (called 'context'), and\n"
                "2) Your general medical knowledge.\n\n"
                "Use BOTH sources to answer the question. If there is any conflict, prioritize the textbook context. "
                "In your answer, clearly say that the information is based on both the reference document and general "
                "medical knowledge. Do NOT give personalized medical advice or treatment recommendations.\n\n"
                f"Context:\n{context_text}"
            )
        else:  # general
            system_content = (
                "You are a plastic surgery educational assistant. The reference document does not contain "
                "sufficiently reliable information to answer the user's question. Answer using ONLY your general "
                "medical knowledge at a high, educational level. Clearly state that this answer is NOT based on the "
                "provided reference document. Do NOT give clinical advice or treatment recommendations for individual "
                "patients."
            )

        # 6) Ana modele mesaj gÃ¶nder (RAG cevabÄ± iÃ§in)
        messages_main = [
            SystemMessage(content=system_content),
            HumanMessage(content=question_en),
        ]

        resp = llm_main.invoke(messages_main)
        answer_en = getattr(resp, "content", str(resp)).strip()
        logger.debug("Ana LLM cevabÄ± alÄ±ndÄ±.")

        # 7) Gerekirse cevabÄ± TÃ¼rkÃ§eye Ã§evir
        final_answer = (
            translate_en_to_tr(answer_en) if original_is_turkish else answer_en
        )

        # 8) Sohbet tarzÄ± kapanÄ±ÅŸ notu
        closing_note = generate_closing_note(
            original_question=question_raw,
            final_answer=final_answer,
            mode=mode,
            original_is_turkish=original_is_turkish,
        )

        if closing_note:
            final_answer_with_note = f"{final_answer}\n\n{closing_note}"
        else:
            final_answer_with_note = final_answer

        return {
            "answer": final_answer_with_note,
            "pages": page_numbers,          # 0-based index
            "confidence": round(confidence, 3),
            "source_count": len(reranked_docs),
            "mode": mode,
            "sources": sources_list,        # kaynak dokÃ¼man listesi
        }

    retrieval_chain = RunnableLambda(rag_fn)

    logger.info(
        "RAG zinciri baÅŸarÄ±yla kuruldu (TR/EN, 3-seviyeli confidence, reranker + sohbet kapanÄ±ÅŸlÄ±, "
        "temperature=%.2f, retriever_k=%d, max_tokens=%s).",
        effective_temperature,
        effective_retriever_k,
        str(max_tokens),
    )
    return retrieval_chain
